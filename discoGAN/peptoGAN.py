from __future__ import print_function
# -*- coding: utf-8 -*-
"""DiscoGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KxZWFd70-sVXnsWBVp7a5wku1I_-KYMd

When you train the discriminator, hold the generator values constant; and when you train the generator, hold the discriminator constant. Each should train against a static adversary. For example, this gives the generator a better read on the gradient it must learn by.
"""

from PIL import Image

# original = Image.open("file.ppm") # load an image from the hard drive


import os
from os.path import join, exists
import multiprocessing
import hashlib
# import cv2
import sys
import zipfile
import argparse
from six.moves import urllib

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
# import ipdb

import numpy as np

from itertools import chain

import scipy
from progressbar import ETA, Bar, Percentage, ProgressBar

# from google.colab import drive
# drive.mount('/content/gdrive/')

import pickle

x = pickle.load(open("x_test.pickle",'rb'))
y = pickle.load(open("y_test.pickle",'rb'))

# print(x[0])
# print("******************************************************************************")
# j = 0
threeDList = []
for image in x:
    threeDImage = np.reshape(image, (-1,25,8))
    threeDList.append(threeDImage)
    # if j == 0:
        # print(threeDImage)
        # print(threeDImage.shape)
    # j = j + 1
# print(threeDList[0])
threeDList = np.asarray(threeDList)
# print(threeDList.shape)
x = threeDList

zero = []
one = []
two = []

for imageIndex in range(len(x)):
  x[imageIndex] = Variable( torch.FloatTensor( x[imageIndex] ))
  if y[imageIndex] == 0:
    zero.append(x[imageIndex])
  elif y[imageIndex] == 1:
    one.append(x[imageIndex])
  else:
    two.append(x[imageIndex])
    
print(len(zero))
print(len(one))
print(len(two))

# Make a dataLoader object



kernel_sizes = [4,3,3] # we will want to change this to match the sizes of the peptide images (and they need to run the lengths of the columns) (also, we only have one color, so only 1 channel)
strides = [2,2,1]
paddings=[0,0,1]

latent_dim = 300

"""#### Descriiminator Class
The descriminator is simply a convolutional neural network that classifies output from the generator. In our case, the descriminator will classify peptides as belonging to a particular class of toxicity or not (1 or 0).
"""

class Discriminator(nn.Module):
    def __init__(
            self,
            ):

        super(Discriminator, self).__init__() # allows you to access nn.Module as a parent class
        # This uses the python 2 version of super. The new way to use super is: super()

        print("XXX")
        # Layer 1
        # takes 3 input channels (the 3 RGB color values), gives 64 output chanels, with a kernal size of 2, and a stride of 1
        self.conv1 = nn.Conv2d(1,10, kernel_size = (25,2), bias=False) # in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True
        # This doesn't have a batch normalization layer because it assumes your input is normalized already (and the peptide pictures should come normalized)
        self.relu1 = nn.LeakyReLU(0.2, inplace=True) # This is the part of the layer that actually transforms inputs to outputs.

        # Layer 2
        # The input channels from here comes from the output channel of layer1 (size 64)
        # Gives an output of the number of input channels multiplied by an arbitrary number (however much you want to grow the network) (64 * 2 here)
        self.conv2 = nn.Conv2d(200, 200 * 2, 4, 2, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(200 * 2) # oils the neural network. The parameter entered is num_features. 
        self.relu2 = nn.LeakyReLU(0.2, inplace=True) # ReLU stands for Rectified linear. It is a non-linear function that looks like a linear function, and is the most common activation function in CNNs.

        # Layer 3
        self.conv3 = nn.Conv2d(200 * 2, 200 * 4, 4, 2, 1, bias=False)
        self.bn3 = nn.BatchNorm2d(200 * 4) # this reduces the amount of dropout we need in the leaky part ofthe ReLu function.
        self.relu3 = nn.LeakyReLU(0.2, inplace=True) # the leaky coefficient helps avoid overfitting. It drops a certain % of the weights.

        # Layer 4
        self.conv4 = nn.Conv2d(200 * 4, 200 * 8, 4, 2, 1, bias=False)
        self.bn4 = nn.BatchNorm2d(200 * 8)
        self.relu4 = nn.LeakyReLU(0.2, inplace=True)

         # Layer 5: Output layer
        self.conv5 = nn.Conv2d(200 * 8, 1, 4, 1, 0, bias=False) # reduces from 64*8 layers to only 1 layer, a 1 or 0 stating whether or not the image passed.

    def forward(self, input): # run input through the layers (perform a forward pass to get an output)
        print("WWW")
        # run the input layer
        conv1 = self.conv1( input )
        relu1 = self.relu1( conv1 )

        # run output from layer 1 through layer 2
        conv2 = self.conv2( relu1 ) # read data in to layer
        bn2 = self.bn2( conv2 ) # normalize the input before transforming it
        relu2 = self.relu2( bn2 ) # transform the input to outputs
  
        # run output from layer 2 through layer 3
        conv3 = self.conv3( relu2 )
        bn3 = self.bn3( conv3 )
        relu3 = self.relu3( bn3 )
        
        # run output from layer 3 through layer 4
        conv4 = self.conv4( relu3 )
        bn4 = self.bn4( conv4 )
        relu4 = self.relu4( bn4 )
        
        # run output through the final layer
        conv5 = self.conv5( relu4 )
        
        # use a signmoid function to classify the output from lyaer 5.
        return torch.sigmoid( conv5 ), [relu2, relu3, relu4] # return intermediate layer values, too (Why? Not sure yet.)

"""#### Genrator Class
This deconvolutional neural network will learn to produce peptide images. These images will be passed to the descriminator, and the generator will evaluate its effectiveness on how well it fools the descriminator.
"""

class Generator(nn.Module):
    stride = (1,1)
    padding = (12,4)
    dilation = (1,2)
    kernel_size = (25,2)


    def __init__(
            self,

            extra_layers=False
            ):
      
        # allows us to inherit from nn.Module as a parent class
        super(Generator, self).__init__()
        if extra_layers == True:
            self.main = nn.Sequential(
                print("ZZZZ"),
                # TODO: Change to match size of our images
                # This part reads in the image and transforms it into 100 numbers
                nn.Conv2d(in_channels=1, out_channels=200, kernel_size=(3,5), stride=2, padding=1, bias=False),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(200, 200 * 2, 4, 2, 1, bias=False),
                nn.BatchNorm2d(200 * 2),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(200 * 2, 200 * 4, 4, 2, 1, bias=False),
                nn.BatchNorm2d(200 * 4),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(200 * 4, 200 * 8, 4, 2, 1, bias=False),
                nn.BatchNorm2d(200 * 8),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(200 * 8, 100, 4, 1, 0, bias=False),
                nn.BatchNorm2d(100),
                nn.LeakyReLU(0.2, inplace=True),

                # this part takes the transormed image (100 numbers) and tries to put it back together
                nn.ConvTranspose2d(100, 200 * 8, 4, 1, 0, bias=False),
                nn.BatchNorm2d(200 * 8),
                nn.ReLU(True),
                nn.ConvTranspose2d(200 * 8, 200 * 4, 4, 2, 1, bias=False),
                nn.BatchNorm2d(200 * 4),
                nn.ReLU(True),
                nn.ConvTranspose2d(200 * 4, 200 * 2, 4, 2, 1, bias=False),
                nn.BatchNorm2d(200 * 2),
                nn.ReLU(True),
                nn.ConvTranspose2d(200 * 2,     200, 4, 2, 1, bias=False),
                nn.BatchNorm2d(200),
                nn.ReLU(True),
                nn.ConvTranspose2d(    200,      1, 4, 2, 1, bias=False),
                nn.Sigmoid()
            )


        if extra_layers == False:
            print("YYY")
            self.main = nn.Sequential(
                nn.Conv2d(in_channels=1, out_channels=200, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding,dilation=self.dilation, bias=False),
                # nn.Conv2d(1, 200, 4, 2, 1, bias=False),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(200, 200 * 2, self.kernel_size, self.stride, self.padding, self.dilation, bias=False),
                nn.BatchNorm2d(200 * 2),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(200 * 2, 200 * 4, self.kernel_size, self.stride, self.padding, self.dilation, bias=False),
                nn.BatchNorm2d(200 * 4),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(200 * 4, 200 * 8, self.kernel_size, self.stride, self.padding, self.dilation, bias=False),
                nn.BatchNorm2d(200 * 8),
                nn.LeakyReLU(0.2, inplace=True),

                nn.ConvTranspose2d(200 * 8, 200 * 4, (25,2), 1, 1, bias=False),
                nn.BatchNorm2d(200 * 4),
                nn.ReLU(True),
                nn.ConvTranspose2d(200 * 4, 200 * 2, (25,2), 1, 1, bias=False),
                nn.BatchNorm2d(200 * 2),
                nn.ReLU(True),
                nn.ConvTranspose2d(200 * 2,     200, (25,2), 1, 1, bias=False),
                nn.BatchNorm2d(200),
                nn.ReLU(True),
                nn.ConvTranspose2d(    200,      1, (25,2), 1, 1, bias=False),
                nn.Sigmoid()
            )

    def forward(self, input):
        print("forward in generator")
        return self.main( input )

# Run Facescrub dataset (gender translation) arguments

# script.sh
# python ./discogan/image_translation.py --task_name='facescrub' --batch_size=500

# image_translation.py main()

# Add arguments to parser
parser = argparse.ArgumentParser(description='PyTorch implementation of DiscoGAN')
parser.add_argument('--cuda', type=str, default='true', help='Set cuda usage')
parser.add_argument('--task_name', type=str, default='facescrub', help='Set data name')
parser.add_argument('--epoch_size', type=int, default=5000, help='Set epoch size')
parser.add_argument('--batch_size', type=int, default=64, help='Set batch size')
parser.add_argument('--learning_rate', type=float, default=0.0002, help='Set learning rate for optimizer')
parser.add_argument('--result_path', type=str, default='./results/', help='Set the path the result images will be saved.')
parser.add_argument('--model_path', type=str, default='./models/', help='Set the path for trained models')
parser.add_argument('--model_arch', type=str, default='discogan', help='choose among gan/recongan/discogan. gan - standard GAN, recongan - GAN with reconstruction, discogan - DiscoGAN.')
parser.add_argument('--image_size', type=int, default=64, help='Image size. 64 for every experiment in the paper')

parser.add_argument('--gan_curriculum', type=int, default=10000, help='Strong GAN loss for certain period at the beginning')
parser.add_argument('--starting_rate', type=float, default=0.01, help='Set the lambda weight between GAN loss and Recon loss during curriculum period at the beginning. We used the 0.01 weight.')
parser.add_argument('--default_rate', type=float, default=0.5, help='Set the lambda weight between GAN loss and Recon loss after curriculum period. We used the 0.5 weight.')

parser.add_argument('--style_A', type=str, default=None, help='Style for CelebA dataset. Could be any attributes in celebA (Young, Male, Blond_Hair, Wearing_Hat ...)')
parser.add_argument('--style_B', type=str, default=None, help='Style for CelebA dataset. Could be any attributes in celebA (Young, Male, Blond_Hair, Wearing_Hat ...)')
parser.add_argument('--constraint', type=str, default=None, help='Constraint for celebA dataset. Only images satisfying this constraint is used. For example, if --constraint=Male, and --constraint_type=1, only male images are used for both style/domain.')
parser.add_argument('--constraint_type', type=str, default=None, help='Used along with --constraint. If --constraint_type=1, only images satisfying the constraint are used. If --constraint_type=-1, only images not satisfying the constraint are used.')
parser.add_argument('--n_test', type=int, default=200, help='Number of test data.')

parser.add_argument('--update_interval', type=int, default=3, help='')
parser.add_argument('--log_interval', type=int, default=50, help='Print loss values every log_interval iterations.')
parser.add_argument('--image_save_interval', type=int, default=1000, help='Save test results every image_save_interval iterations.')
parser.add_argument('--model_save_interval', type=int, default=10000, help='Save models every model_save_interval iterations.')

"""The facescrub file that is in the repo is datasets/facescrub_actors.txt or datasets/facescrub_actresses.txt.

The file format is:

    name	image_id	face_id	url	bbox	sha256
    
 Example:
 
    Aaron Eckhart	1	1	http://upload.wikimedia.org/wikipedia/commons/5/5d/AaronEckhart10TIFF.jpg	53,177,418,542	dec996994cf1eec33b53c203cff0e8f25638829fa2ad71bb0307d308fa11cdac
"""

# dataset.py
# Called from get_data() in image_translation.py
dataset_path = './datasets/'
facescrub_path = os.path.join(dataset_path, 'facescrub')

def get_facescrub_files(test=False, n_test=200):
    actor_path = os.path.join(facescrub_path, 'actors', 'face' ) # This file doesn't exist in the repo, but I think we just need the path to our images
    actress_path = os.path.join( facescrub_path, 'actresses', 'face' ) # See comment and note above

    actor_files = map(lambda x: os.path.join( actor_path, x ), os.listdir( actor_path ) )
    actress_files = map(lambda x: os.path.join( actress_path, x ), os.listdir( actress_path ) )

    if test == False:
        return actor_files[:-n_test], actress_files[:-n_test]  # Returns all samples except last n_test samples
    else:
        return actor_files[-n_test:], actress_files[-n_test:]  # Returns last n_test samples

# image_translation.py
# Called from main() in image_translation.py

def get_data():
  
  zero = [] # antitoxic
  one = [] # toxic
  two = [] # neutral


  for imageIndex in range(len(x)):
    # x[imageIndex] = Variable( torch.FloatTensor( x[imageIndex] ), volatile=True )
    if y[imageIndex] == 0:
      zero.append(Variable( torch.FloatTensor( x[imageIndex] )))
    elif y[imageIndex] == 1:
      one.append(Variable( torch.FloatTensor( x[imageIndex] )))
    else:
      two.append(Variable( torch.FloatTensor( x[imageIndex] )))
      
#     if args.task_name == 'facescrub':
#         data_A, data_B = get_facescrub_files(test=False, n_test=args.n_test)
#         test_A, test_B = get_facescrub_files(test=True, n_test=args.n_test)
    
    # I ommitted the rest of the data extraction stuff for the other tasks. facescrub is simplest and possibly closest to what we need to do
  data_A = zero[1:]
  data_B = one[1:]
  test_A = zero[0]
  test_B = one[0]
  return data_A, data_B, test_A, test_B


def shuffle_data(da, db):
    a_idx = list(range(len(da)))
    np.random.shuffle( a_idx )

    b_idx = list(range(len(db)))
    np.random.shuffle(b_idx)

    shuffled_da = []
    for index in a_idx:
        shuffled_da.append(da[index])
    shuffled_db = []
    for index in b_idx:
        shuffled_db.append(db[index])
    # shuffled_da = np.array(da)[ np.array(a_idx) ]
    # shuffled_db = np.array(db)[ np.array(b_idx) ]

    return shuffled_da, shuffled_db
def get_fm_loss(real_feats, fake_feats, criterion):
    losses = 0
    for real_feat, fake_feat in zip(real_feats[1:], fake_feats[1:]):
        l2 = (real_feat.mean(0) - fake_feat.mean(0)) * (real_feat.mean(0) - fake_feat.mean(0))
        loss = criterion( l2, Variable( torch.ones( l2.size() ) ).cuda() )
        losses += loss

    return losses

def get_gan_loss(dis_real, dis_fake, criterion, cuda):
    labels_dis_real = Variable(torch.ones( [dis_real.size()[0], 1] ))
    labels_dis_fake = Variable(torch.zeros([dis_fake.size()[0], 1] ))
    labels_gen = Variable(torch.ones([dis_fake.size()[0], 1]))

    if cuda:
        labels_dis_real = labels_dis_real.cuda()
        labels_dis_fake = labels_dis_fake.cuda()
        labels_gen = labels_gen.cuda()

    dis_loss = criterion( dis_real, labels_dis_real ) * 0.5 + criterion( dis_fake, labels_dis_fake ) * 0.5
    gen_loss = criterion( dis_fake, labels_gen )

    return dis_loss, gen_loss

def as_np(data):
    return data.cpu().data.numpy()


# image_translation.py
# Called at runtime

def getBatch(data, index, batchSize):
    # newBatch = []
    # i = index - batchSize
    # while i < index:
    #     newBatch.append(data[i])
    #     i = i + 1
    print("******************")
    print(type(data))
    newBatch = data[index * batchSize: (index + 1) * batchSize]
    newBatch = np.stack(newBatch)
    newBatch = Variable( torch.FloatTensor(newBatch))
    # tensorBatch = torch.FloatTensor(newBatch)
    return newBatch

def main():

    cuda = False
    learning_rate = 0.0002
    model_save_interval = 10000
    image_save_interval = 10000
    update_interval = 3
    log_interval = 50
    gan_curriculum = 10000
    starting_rate = 0.01
    default_rate = 0.5

    task_name = "anti2Toxic"

    epoch_size = 3
    batch_size = 30

    result_path = "test1"
    model_path = "test1"

    data_A, data_B, test_A, test_B = get_data()

    # Setting up Networks
    generator_A = Generator()
    generator_B = Generator()
    discriminator_A = Discriminator()
    discriminator_B = Discriminator()

    if cuda:
        test_A = test_A.cuda()
        test_B = test_B.cuda()
        generator_A = generator_A.cuda()
        generator_B = generator_B.cuda()
        discriminator_A = discriminator_A.cuda()
        discriminator_B = discriminator_B.cuda()

    data_size = min( len(data_A), len(data_B) )
    n_batches = ( data_size // batch_size )

    # Set up different loss functions
    recon_criterion = nn.MSELoss()
    gan_criterion = nn.BCELoss()
    feat_criterion = nn.HingeEmbeddingLoss()

    # If we want to change the parameters
    gen_params = chain(generator_A.parameters(), generator_B.parameters())
    dis_params = chain(discriminator_A.parameters(), discriminator_B.parameters())

    # Setting up gradient descent
    optim_gen = optim.Adam( gen_params, lr= learning_rate, betas=(0.5,0.999), weight_decay=0.00001)  # Default learning_rate is 0.0002
    optim_dis = optim.Adam( dis_params, lr= learning_rate, betas=(0.5,0.999), weight_decay=0.00001)

    iters = 0

    gen_loss_total = []
    dis_loss_total = []

    imageIndex = 0
    
    for epoch in range(epoch_size):
        data_style_A, data_style_B = shuffle_data( data_A, data_B)

        # Progression bar
        widgets = ['epoch #%d|' % epoch, Percentage(), Bar(), ETA()]
        pbar = ProgressBar(maxval=n_batches, widgets=widgets)
        pbar.start()

        for i in range(n_batches):

            pbar.update(i)

            # Reset gradient_
            generator_B.zero_grad()
            discriminator_A.zero_grad()
            discriminator_B.zero_grad()

            if imageIndex > len(data_A):
              break

            # This returns a batch of dimension batch_size, in_chanels, height, width (30,1,25,8)
            A = getBatch(data_A, imageIndex, batch_size)
            # A = data_A[imageIndex]
            B = getBatch(data_B, imageIndex, batch_size)
            # B = data_B[imageIndex]
  
  
            if cuda:
                A = A.cuda()
                B = B.cuda()

            # Generator B turns A into B
            AB = generator_B(A)
            BA = generator_A(B)

            ABA = generator_A(AB)  # Should be back to original images
            BAB = generator_B(BA)

            # Reconstruction Loss, How successful was reconstruction
            recon_loss_A = recon_criterion( ABA, A )
            recon_loss_B = recon_criterion( BAB, B )

            # Real/Fake GAN Loss (A)
            A_dis_real, A_feats_real = discriminator_A( A )
            A_dis_fake, A_feats_fake = discriminator_A( BA )

            dis_loss_A, gen_loss_A = get_gan_loss( A_dis_real, A_dis_fake, gan_criterion, cuda )
            fm_loss_A = get_fm_loss(A_feats_real, A_feats_fake, feat_criterion)

            # Real/Fake GAN Loss (B)
            B_dis_real, B_feats_real = discriminator_B( B )
            B_dis_fake, B_feats_fake = discriminator_B( AB )

            dis_loss_B, gen_loss_B = get_gan_loss( B_dis_real, B_dis_fake, gan_criterion, cuda )
            fm_loss_B = get_fm_loss( B_feats_real, B_feats_fake, feat_criterion )

            # Total Loss

            if iters < gan_curriculum:
                rate = starting_rate
            else:
                rate = default_rate

            gen_loss_A_total = (gen_loss_B*0.1 + fm_loss_B*0.9) * (1.-rate) + recon_loss_A * rate
            gen_loss_B_total = (gen_loss_A*0.1 + fm_loss_A*0.9) * (1.-rate) + recon_loss_B * rate

            # if args.model_arch == 'discogan':  # This is what we want
            gen_loss = gen_loss_A_total + gen_loss_B_total
            dis_loss = dis_loss_A + dis_loss_B
            # elif args.model_arch == 'recongan':
            #     gen_loss = gen_loss_A_total
            #     dis_loss = dis_loss_B
            # elif args.model_arch == 'gan':
            #     gen_loss = (gen_loss_B*0.1 + fm_loss_B*0.9)
            #     dis_loss = dis_loss_B

            if iters % update_interval == 0:
                dis_loss.backward()  # Updating network
                optim_dis.step()
            else:
                gen_loss.backward()
                optim_gen.step()

            if iters % log_interval == 0:
                print("---------------------")
                print("GEN Loss:", as_np(gen_loss_A.mean()), as_np(gen_loss_B.mean()))
                print("Feature Matching Loss:", as_np(fm_loss_A.mean()), as_np(fm_loss_B.mean()))
                print("RECON Loss:", as_np(recon_loss_A.mean()), as_np(recon_loss_B.mean()))
                print("DIS Loss:", as_np(dis_loss_A.mean()), as_np(dis_loss_B.mean()))

            # run the test set if we are on the first round
            if iters % image_save_interval == 0:
                AB = generator_B( test_A )
                BA = generator_A( test_B )
                ABA = generator_A( AB )
                BAB = generator_B( BA )

                n_testset = min( test_A.size()[0], test_B.size()[0] )

                subdir_path = os.path.join( result_path, str(iters / image_save_interval) )

                if os.path.exists( subdir_path ):
                    pass
                else:
                    os.makedirs( subdir_path )

                for im_idx in range( n_testset ):
                    A_val = test_A[im_idx].cpu().data.numpy().transpose(1,2,0) * 255.
                    B_val = test_B[im_idx].cpu().data.numpy().transpose(1,2,0) * 255.
                    BA_val = BA[im_idx].cpu().data.numpy().transpose(1,2,0)* 255.
                    ABA_val = ABA[im_idx].cpu().data.numpy().transpose(1,2,0)* 255.
                    AB_val = AB[im_idx].cpu().data.numpy().transpose(1,2,0)* 255.
                    BAB_val = BAB[im_idx].cpu().data.numpy().transpose(1,2,0)* 255.

                    filename_prefix = os.path.join (subdir_path, str(im_idx))
                    scipy.misc.imsave( filename_prefix + '.A.jpg', A_val.astype(np.uint8)[:,:,::-1])
                    scipy.misc.imsave( filename_prefix + '.B.jpg', B_val.astype(np.uint8)[:,:,::-1])
                    scipy.misc.imsave( filename_prefix + '.BA.jpg', BA_val.astype(np.uint8)[:,:,::-1])
                    scipy.misc.imsave( filename_prefix + '.AB.jpg', AB_val.astype(np.uint8)[:,:,::-1])
                    scipy.misc.imsave( filename_prefix + '.ABA.jpg', ABA_val.astype(np.uint8)[:,:,::-1])
                    scipy.misc.imsave( filename_prefix + '.BAB.jpg', BAB_val.astype(np.uint8)[:,:,::-1])

            # save at teh save interval
            if iters % model_save_interval == 0:
                torch.save( generator_A, os.path.join(model_path, 'model_gen_A-' + str( iters / model_save_interval )))
                torch.save( generator_B, os.path.join(model_path, 'model_gen_B-' + str( iters / model_save_interval )))
                torch.save( discriminator_A, os.path.join(model_path, 'model_dis_A-' + str( iters / model_save_interval )))
                torch.save( discriminator_B, os.path.join(model_path, 'model_dis_B-' + str( iters / model_save_interval )))

            iters += 1
            imageIndex = imageIndex + 1


stride = (1,1)
padding = (1,1)
dilation = (1,1)
kernel_size = (5,3)
output_padding = (0,0)
#nn.Conv2d(3, 64, 4, 2, 1, bias=False)

def getHout(Hin):
    Hout = (((Hin + 2*padding[0]) - (dilation[0] * (kernel_size[0] - 1)) - 1) / stride[0]) + 1
    return Hout

def getWout(Win):
    Wout = (((Win + 2*padding[1]) - (dilation[1] * (kernel_size[1] - 1)) - 1) / stride[1]) + 1
    return Wout


# Covolution 1
Hout = getHout(25)
Wout = getWout(8)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 2
Hout = getHout(Hout)
Wout = getWout(Hout)

print("************")
print(Hout)
print(Wout)
print("**********************")

#3
Hout = getHout(Hout)
Wout = getWout(Hout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 4
Hout = getHout(Hout)
Wout = getWout(Hout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 5
Hout = getHout(Hout)
Wout = getWout(Hout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 6
Hout = getHout(Hout)
Wout = getWout(Hout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 7
Hout = getHout(Hout)
Wout = getWout(Hout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 8
Hout = getHout(Hout)
Wout = getWout(Hout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 9
Hout = getHout(Hout)
Wout = getWout(Hout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 10
Hout = getHout(Hout)
Wout = getWout(Hout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 11
Hout = getHout(Hout)
Wout = getWout(Hout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 12
Hout = getHout(Hout)
Wout = getWout(Hout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# if we do 12 convolutions with the following settings, we can boil it down ot a single number:
# stride = (1,1)
# padding = (1,1)
# dilation = (1,1)
# kernel_size = (5,3)
# output_padding = (0,0)
# we basically have an autoencoder if we do that . . .



# stride = (1,1)
# padding = (12,4)
# dilation = (1,2)
# kernel_size = (25,2)
# output_padding = (0,0)

def getTransHout(Hin):
    Hout = (Hin - 1) * stride[0] - (2 * padding[0]) + (dilation[0] * (kernel_size[0] - 1)) + output_padding[0] + 1
    return Hout

def getTransWout(Win):
    Wout = (Win - 1) * stride[1] - (2 * padding[1]) + (dilation[1] * (kernel_size[1] - 1)) + output_padding[1] + 1
    return Wout


# deconvolution 1
Hout = getTransHout(1)
Wout = getTransWout(1)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 2
Hout = getTransHout(Hout)
Wout = getTransWout(Wout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 3
Hout = getTransHout(Hout)
Wout = getTransWout(Wout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 4
Hout = getTransHout(Hout)
Wout = getTransWout(Wout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 5
Hout = getTransHout(Hout)
Wout = getTransWout(Wout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 6
Hout = getTransHout(Hout)
Wout = getTransWout(Wout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 7
Hout = getTransHout(Hout)
Wout = getTransWout(Wout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 8
Hout = getTransHout(Hout)
Wout = getTransWout(Wout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 9
Hout = getTransHout(Hout)
Wout = getTransWout(Wout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 10
Hout = getTransHout(Hout)
Wout = getTransWout(Wout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 11
Hout = getTransHout(Hout)
Wout = getTransWout(Wout)

print("************")
print(Hout)
print(Wout)
print("**********************")

# 12
Hout = getTransHout(Hout)
Wout = getTransWout(Wout)

print("************")
print(Hout)
print(Wout)
print("**********************")







# FIXME: Main is here main
# main()










# if __name__=="__main__":
#     main()

# From Cole's code
# class CelebaDataset(Dataset):
#   def __init__(self, root, size=128, train=True):
    
    # super(CelebaDataset, self).__init__() # allows us to inherit from Dataset as a parent
    #
    # self.dataset_folder = torchvision.datasets.ImageFolder(os.path.join(root) ,transform = transforms.Compose([transforms.Resize((size,size)),transforms.ToTensor()]))
  # def __getitem__(self,index):
  #   img = self.dataset_folder[index + 92]
  #   return img[0]
  #
  # def __len__(self):
  #   return int((len(self.dataset_folder) - 93) / 2)
